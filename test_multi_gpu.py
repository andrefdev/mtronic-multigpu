"""
Script de prueba r√°pida para verificar el sistema Multi-GPU BGE-M3
Ejecuta tests b√°sicos para confirmar que todo est√° configurado correctamente
"""

import sys
import time
import traceback
from pathlib import Path

def test_imports():
    """Verifica que todas las dependencias est√©n disponibles"""
    print("üîç Verificando imports...")
    
    try:
        import torch
        print(f"‚úÖ PyTorch: {torch.__version__}")
        
        import transformers
        print(f"‚úÖ Transformers: {transformers.__version__}")
        
        from FlagEmbedding import BGEM3FlagModel
        print("‚úÖ FlagEmbedding: Disponible")
        
        import psutil
        print(f"‚úÖ PSUtil: {psutil.__version__}")
        
        import numpy as np
        print(f"‚úÖ NumPy: {np.__version__}")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Error de import: {e}")
        return False

def test_cuda():
    """Verifica CUDA y GPUs"""
    print("\nüöÄ Verificando CUDA y GPUs...")
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("‚ùå CUDA no disponible")
            return False
        
        gpu_count = torch.cuda.device_count()
        print(f"‚úÖ GPUs detectadas: {gpu_count}")
        
        if gpu_count < 4:
            print(f"‚ö†Ô∏è  Solo {gpu_count} GPUs detectadas, se esperaban 4")
        
        # Informaci√≥n detallada de cada GPU
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            print(f"  GPU {i}: {props.name} - {memory_gb:.1f}GB VRAM")
            
            # Verificar que es L40S
            if "L40S" in props.name:
                print(f"    ‚úÖ L40S detectada")
            else:
                print(f"    ‚ö†Ô∏è  No es L40S: {props.name}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error verificando CUDA: {e}")
        return False

def test_memory_allocation():
    """Test b√°sico de asignaci√≥n de memoria GPU"""
    print("\nüíæ Verificando asignaci√≥n de memoria GPU...")
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("‚ùå CUDA no disponible para test de memoria")
            return False
        
        gpu_count = torch.cuda.device_count()
        
        for gpu_id in range(min(gpu_count, 4)):
            torch.cuda.set_device(gpu_id)
            
            # Asignar tensor peque√±o
            x = torch.randn(1000, 1000, device=f"cuda:{gpu_id}", dtype=torch.float16)
            y = torch.randn(1000, 1000, device=f"cuda:{gpu_id}", dtype=torch.float16)
            
            # Operaci√≥n simple
            z = torch.mm(x, y)
            
            # Verificar memoria
            allocated = torch.cuda.memory_allocated(gpu_id) / (1024**3)
            reserved = torch.cuda.memory_reserved(gpu_id) / (1024**3)
            
            print(f"  GPU {gpu_id}: Test exitoso - {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")
            
            # Limpiar
            del x, y, z
            torch.cuda.empty_cache()
        
        print("‚úÖ Test de memoria GPU exitoso")
        return True
        
    except Exception as e:
        print(f"‚ùå Error en test de memoria: {e}")
        return False

def test_bge_model():
    """Test de carga del modelo BGE-M3"""
    print("\nü§ñ Verificando carga del modelo BGE-M3...")
    
    try:
        import torch
        from FlagEmbedding import BGEM3FlagModel
        
        if not torch.cuda.is_available():
            print("‚ùå CUDA no disponible para test de modelo")
            return False
        
        print("  Cargando modelo BGE-M3...")
        start_time = time.time()
        
        model = BGEM3FlagModel(
            "BAAI/bge-m3",
            use_fp16=True,
            device="cuda:0",
            model_kwargs={
                "torch_dtype": torch.float16,
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
            },
        )
        
        load_time = time.time() - start_time
        print(f"  ‚úÖ Modelo cargado en {load_time:.1f}s")
        
        # Test de embedding simple
        test_texts = ["Este es un texto de prueba.", "Another test sentence."]
        
        print("  Generando embeddings de prueba...")
        start_time = time.time()
        
        embeddings = model.encode(
            sentences=test_texts,
            batch_size=2,
            max_length=512,
            return_dense=True,
            return_sparse=False,
            return_colbert_vecs=False,
        )["dense_vecs"]
        
        inference_time = time.time() - start_time
        print(f"  ‚úÖ Embeddings generados en {inference_time:.3f}s")
        print(f"  üìä Shape: {embeddings.shape}")
        
        # Limpiar memoria
        del model
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error cargando modelo BGE-M3: {e}")
        traceback.print_exc()
        return False

def test_multi_gpu_allocation():
    """Test de asignaci√≥n en m√∫ltiples GPUs"""
    print("\nüîÑ Verificando asignaci√≥n multi-GPU...")
    
    try:
        import torch
        import threading
        import queue
        
        if not torch.cuda.is_available():
            print("‚ùå CUDA no disponible")
            return False
        
        gpu_count = torch.cuda.device_count()
        test_gpus = min(gpu_count, 4)
        
        def gpu_worker(gpu_id, result_queue):
            try:
                torch.cuda.set_device(gpu_id)
                
                # Operaci√≥n en GPU
                x = torch.randn(2000, 2000, device=f"cuda:{gpu_id}", dtype=torch.float16)
                y = torch.randn(2000, 2000, device=f"cuda:{gpu_id}", dtype=torch.float16)
                
                start_time = time.time()
                z = torch.mm(x, y)
                gpu_time = time.time() - start_time
                
                # Verificar resultado
                result_sum = z.sum().item()
                
                result_queue.put({
                    "gpu_id": gpu_id,
                    "success": True,
                    "time": gpu_time,
                    "result_sum": result_sum
                })
                
                # Limpiar
                del x, y, z
                torch.cuda.empty_cache()
                
            except Exception as e:
                result_queue.put({
                    "gpu_id": gpu_id,
                    "success": False,
                    "error": str(e)
                })
        
        # Ejecutar en paralelo
        result_queue = queue.Queue()
        threads = []
        
        start_time = time.time()
        
        for gpu_id in range(test_gpus):
            thread = threading.Thread(target=gpu_worker, args=(gpu_id, result_queue))
            threads.append(thread)
            thread.start()
        
        # Esperar resultados
        for thread in threads:
            thread.join(timeout=30)
        
        total_time = time.time() - start_time
        
        # Recoger resultados
        results = []
        while not result_queue.empty():
            results.append(result_queue.get())
        
        # Analizar resultados
        successful_gpus = [r for r in results if r["success"]]
        failed_gpus = [r for r in results if not r["success"]]
        
        print(f"  ‚úÖ GPUs exitosas: {len(successful_gpus)}/{test_gpus}")
        print(f"  ‚è±Ô∏è  Tiempo total: {total_time:.2f}s")
        
        for result in successful_gpus:
            print(f"    GPU {result['gpu_id']}: {result['time']:.3f}s")
        
        if failed_gpus:
            print("  ‚ùå GPUs con errores:")
            for result in failed_gpus:
                print(f"    GPU {result['gpu_id']}: {result['error']}")
        
        return len(successful_gpus) == test_gpus
        
    except Exception as e:
        print(f"‚ùå Error en test multi-GPU: {e}")
        return False

def test_async_capabilities():
    """Test b√°sico de capacidades as√≠ncronas"""
    print("\nüîÑ Verificando capacidades as√≠ncronas...")
    
    try:
        import asyncio
        import aiohttp
        
        async def simple_async_test():
            # Test b√°sico de asyncio
            await asyncio.sleep(0.1)
            return "async_ok"
        
        # Ejecutar test
        result = asyncio.run(simple_async_test())
        
        if result == "async_ok":
            print("  ‚úÖ AsyncIO funcionando correctamente")
            return True
        else:
            print("  ‚ùå Error en AsyncIO")
            return False
            
    except Exception as e:
        print(f"‚ùå Error en test async: {e}")
        return False

def test_configuration_files():
    """Verifica que los archivos de configuraci√≥n existan"""
    print("\nüìÅ Verificando archivos de configuraci√≥n...")
    
    files_to_check = [
        "bge_m3_multi_gpu_optimized.py",
        "requirements_multi_gpu.txt",
        "setup_multi_gpu.py",
        "gpu_monitor.py",
        "README_multi_gpu.md"
    ]
    
    current_dir = Path(__file__).parent
    all_exist = True
    
    for filename in files_to_check:
        file_path = current_dir / filename
        if file_path.exists():
            size_kb = file_path.stat().st_size / 1024
            print(f"  ‚úÖ {filename} - {size_kb:.1f}KB")
        else:
            print(f"  ‚ùå {filename} - NO ENCONTRADO")
            all_exist = False
    
    # Verificar archivos opcionales
    optional_files = [
        "multi_gpu_config.json",
        ".env.multi_gpu",
        "launch_multi_gpu.py"
    ]
    
    print("\n  üìã Archivos opcionales (se crean autom√°ticamente):")
    for filename in optional_files:
        file_path = current_dir / filename
        if file_path.exists():
            print(f"    ‚úÖ {filename}")
        else:
            print(f"    ‚ö†Ô∏è  {filename} - Ser√° creado por setup_multi_gpu.py")
    
    return all_exist

def main():
    """Funci√≥n principal de tests"""
    print("üß™ BGE-M3 MULTI-GPU SYSTEM TEST")
    print("=" * 60)
    print("Verificando configuraci√≥n para 4x L40S GPUs...")
    print("=" * 60)
    
    tests = [
        ("Imports", test_imports),
        ("CUDA y GPUs", test_cuda),
        ("Memoria GPU", test_memory_allocation),
        ("Modelo BGE-M3", test_bge_model),
        ("Multi-GPU", test_multi_gpu_allocation),
        ("Async", test_async_capabilities),
        ("Archivos", test_configuration_files),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
            
            if success:
                print(f"‚úÖ {test_name}: EXITOSO")
            else:
                print(f"‚ùå {test_name}: FALLIDO")
                
        except Exception as e:
            print(f"‚ùå {test_name}: ERROR - {e}")
            results.append((test_name, False))
        
        print()  # L√≠nea en blanco entre tests
    
    # Resumen final
    print("=" * 60)
    print("üìä RESUMEN DE TESTS")
    print("=" * 60)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{test_name:<20} {status}")
    
    print("-" * 40)
    print(f"Total: {passed}/{total} tests exitosos ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nüéâ ¬°TODOS LOS TESTS EXITOSOS!")
        print("üöÄ El sistema est√° listo para procesamiento Multi-GPU")
        print("\nüìã PR√ìXIMOS PASOS:")
        print("1. Ejecutar: python launch_multi_gpu.py")
        print("2. Monitorear: python gpu_monitor.py (terminal separado)")
        print("3. Verificar logs: tail -f bge_m3_multi_gpu.log")
    else:
        failed = total - passed
        print(f"\n‚ö†Ô∏è  {failed} tests fallaron")
        print("üîß Revisar errores antes de continuar")
        print("üí° Ejecutar: python setup_multi_gpu.py para configuraci√≥n autom√°tica")
    
    print("=" * 60)
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
